IAT 360: Exploring Artificial Intelligence
Harjot Singh
October 2, 2024

Graph One (Emotional Balance without my audio):
![IAT360A2GraphOne.png](./IAT360A2GraphOne.png)

Neutral: 94.8
Calm: 190.7
Happy: 190.3
Sad: 190.3
Angry: 190.5
Fearful: 190.7
Disgust: 190.3
Surprised: 189.9

Graph Two (Gender Balance without my audio):
![IAT360A2GraphTwo.png](./IAT360A2GraphTwo.png)

Female: 714.7
Male: 714.8

Graph Three (Emotional Balance WITH my audio):
![IAT360A2GraphThree.png](./IAT360A2GraphThree.png)

Neutral: 97.0
Calm: 190.8
Happy: 192.9
Sad: 192.3
Angry: 193.1
Fearful: 191.3
Disgust: 192.3
Surprised: 191.7

Graph Four (Gender Balance WITH my audio):
![IAT360A2GraphFour.png](./IAT360A2GraphFour.png)

Female: 709.1
Male: 726.6

To analyze these graphs and the data set we must first understand and know what exactly we are analyzing and how the data may differ, and for what reason. Firstly, we ran the data cleaning and processing without my audio files and simply what was provided to us as a default data set. We created graphs for both the emotional balance and gender balance of our data set; after analyzing the values a few things were apparent. We had one outlier in the neutral emotion category, this is likely because it is very difficult for us as humans to say things without any emotion at all, even when we try to do so. A lot of the files most likely got grouped into other emotions such as calm and fearful as those emotions had slightly higher outputs. Furthermore, we were able to see there was a fairly average output for all but the neutral category, with a very small SD, ranging from 189.9-190.7. This tells us that almost all of the audio files in the original data set were accurate enough to be placed into the correct emotional category. 

When we switch over to the gender balance graph there is much less data to analyze, the outputs for the two values, male and female, were again very close with a SD of 0.1, ranging from 714.7-714.8. We can see that the data was cleaned well and was all accurate from the start as we were told that the set has 12 male and 12 female voice actors. From this we can know our data is accurate, clean and reliable in terms of gender balance. We have an even amount of data from both male and female voices.

Now, when I added in my audio files to the data set and ran both graphs again, I was not surprised at all with the outcomes. Firstly, analyzing the emotional balance graph I found very similar results to without my audio files. The outlier again was the neutral category, while all the other emotions stayed in a fairly close range once again. The only slight difference I notice with my audio set is that I must have portrayed certain emotions stronger than others as certain emotions shown at higher output values than the others and than prior. This was most likely due to the way I said the statements and how my emotions maybe were not as accurate as the voice actors. 

Finally, when we look at graph four, the gender balance with my audio files, it is exactly as expected in terms of outcome values. The balance shifted towards a higher male and lower female output. This was fairly intuitive as when analyzing my audio files it will have placed them all into the male gender category and shifted the balance towards that side. 

Overall, I can conclude the data set given was very accurate, clean, and readable. Furthermore, after adding in my audio files I was able to quantify my presumptions of what would happen to our graphs and I did so successfully. 
